-----Setup------

Modules required;
$ module load libs gsl/intel/2.3
$ module load dev intel/2017-u3
(To quickly load these modules just type 'source source_me' (don't include the quotation marks))

Compile command;
$ mpic++ -o run main.cpp functions.cpp density.cpp random.cpp -lgsl -lgslcblas -lm
(on Fionn):
$ mpiicpc -o run main.cpp functions.cpp density.cpp random.cpp -lgsl -lgslcblas -lm

To execute the code;
$ mpirun -n 10 run
(Set the number after '-n' equal to however many processors you want to use)
("run" is the name of the executable. i.e: whatever word you put after '-o' in the compile command)


-----Comments-----

- I have set up a dummy array 'a' to follow the behaviour of the 'sum1' arrays. 
  The 'a' array should end up as an array of length 60 (N_slice), with each element equal to 10000 (Nsample)
  As you can see from the last column in the output files (Output1, Output2 etc.) a is not filled with 10000's

- The 'sum1' arrays seems to give the correct answer when there is only 1 thread (although the 'a' array does not)
  With this in mind I think the data from the master thread is being saved after the parallel region but the data
  from the worker threads is not.

- arrays R1, v and f need to be local to each thread -> memory allocations, definitions, etc.



-----Submitting Jobs-----

- Edit the 'sub.pbs' file to include your executable, the required modules, an email (optional -> will send updates on
  job status)
- Type "qsub sub.pbs" to submit the job
- To view the job status type "qshow"


---- Running using g++ (gcc version 6.3.0)

Compile command;
$ g++ -fopenmp -o run main.cpp density.cpp functions.cpp random.cpp -lgsl -lgslcblas -lm

Run command;
$ export OMP_NUM_THREADS=2; ./run

